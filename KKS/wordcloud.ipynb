{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keyword analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pecab import PeCab\n",
    "from konlpy.tag import Mecab\n",
    "# 워드클라우드 관련\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "# 자연어처리 관련\n",
    "import nltk\n",
    "# 파일 처리 관련\n",
    "import pandas as pd\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mecab 기준, 명사 & 형용사 & 동사 & 부사\n",
    "FEATURE_POS = ['NNG', 'NNP', 'NNB', 'NNBC', 'NR',\n",
    "               'NP', 'VV', 'VA', 'MAG']\n",
    "\n",
    "\n",
    "def text_cleaning(doc):\n",
    "    # 한국어를 제외한 글자를 제거하는 패턴.\n",
    "    #doc = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", doc)\n",
    "    \n",
    "    # 특수문자를 제거하는 패턴.\n",
    "    doc = re.sub(\"[\\{\\}\\[\\]\\/?.,;:|\\)*~`!^\\-_+<>@\\#$%&\\\\\\=\\(\\'\\\"]\", \" \", doc)\n",
    "    \n",
    "    # 영문 빼고 모두 제거하는 패턴.\n",
    "    #doc = doc.replace(\"\\n\", \" \")\n",
    "    #doc = re.sub(\"[^A-Za-z ]\", \"\", doc)\n",
    "    \n",
    "    return doc\n",
    "\n",
    "def define_stopwords(path):\n",
    "    \n",
    "    SW = set()\n",
    "    # 불용어를 추가하는 방법 1.\n",
    "    # SW.add(\"있다\")\n",
    "    SW.add(\"있어요\")\n",
    "    SW.add(\"대한\")\n",
    "    SW.add(\"합니다\")\n",
    "    SW.add(\"하는\")\n",
    "    \n",
    "    # 불용어를 추가하는 방법 2.\n",
    "    # stopwords-ko.txt에 직접 추가\n",
    "    \n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        for word in f:\n",
    "            SW.add(word.strip())\n",
    "            \n",
    "    return SW\n",
    "\n",
    "\n",
    "def text_tokenizing(doc, tokenizer): \n",
    "    \"\"\"\n",
    "    Input Parameter :\n",
    "    \n",
    "    doc - tokenizing 하는 실제 데이터.\n",
    "    tokenizer - token의 단위.\n",
    "    \"\"\"\n",
    "    tok = PeCab() # 형태소 분석기 선언.\n",
    "    \n",
    "    if tokenizer == \"words\":\n",
    "        return [word for word in doc.split() if word not in SW and len(word) > 1]\n",
    "    \n",
    "    elif tokenizer == \"nouns\":\n",
    "        return [token for token in tok.nouns(doc)]\n",
    "        \n",
    "    elif tokenizer == \"morphs\":\n",
    "        return [token for token in tok.morphs(doc)]\n",
    "     \n",
    "    elif tokenizer == \"predefined\":\n",
    "        \n",
    "        documents = []\n",
    "        text_pos = [pair for pair in tok.pos(doc) if pair[0] not in SW and len(pair[0]) > 1]\n",
    "        words = []\n",
    "\n",
    "        for word, pos in text_pos:\n",
    "            if pos in FEATURE_POS:\n",
    "                words.append(word)\n",
    "\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = \"predefined\" # \"words\" / \"nouns\" / \"morphs\" / \"predefined\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords : 불용어 --> token filtering.\n",
    "SW = define_stopwords(\"stopwords-ko.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [공감, 댓글, 공유, 로그인, 소소, 달달, 여행, 일상, 이야기, 스윗, 수련,...\n",
       "1     [공감, 댓글, 공유, 로그인, 블로그, 메뉴, 커피, 여행, 글쓰기, 에디터, 오...\n",
       "2     [공감, 댓글, 공유, 로그인, 블로그, 메뉴, 글쓰기, 에디터, 오픈, 그래픽, ...\n",
       "3     [공감, 댓글, 공유, 로그인, 소소, 꾸준히, 블로그, 메뉴, 글쓰기, 에디터, ...\n",
       "4     [공감, 공유, 로그인, 생각, 블로그, 메뉴, 글쓰기, 에디터, 오픈, 챌린지, ...\n",
       "                            ...                        \n",
       "65    [공감, 댓글, 공유, 로그인, 소소, 꾸준히, 블로그, 메뉴, 글쓰기, 에디터, ...\n",
       "66    [공감, 댓글, 공유, 로그인, 스피릿, 블로그, 블로그, 메뉴, 글쓰기, 에디터,...\n",
       "67    [공감, 댓글, 공유, 로그인, 소소, 꾸준히, 블로그, 메뉴, 글쓰기, 에디터, ...\n",
       "68    [공감, 댓글, 공유, 로그인, 소소, 꾸준히, 블로그, 메뉴, 글쓰기, 에디터, ...\n",
       "69    [공감, 댓글, 공유, 로그인, 소소, 꾸준히, 블로그, 메뉴, 글쓰기, 에디터, ...\n",
       "Name: 0, Length: 70, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = pd.read_csv('./2023-05-11_21-14-14.csv', index_col=0)\n",
    "documents = document['0']\n",
    "tokenized_documents = documents.apply(text_cleaning)\\\n",
    "                               .apply(func=text_tokenizing, tokenizer=tokenizer)\n",
    "tokenized_documents # 2d list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_documents.to_csv(\"tokenized_documents.csv\", encoding=\"utf-8-sig\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word cloud"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yeardream",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
